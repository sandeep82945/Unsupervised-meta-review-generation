# -*- coding: utf-8 -*-
"""Aspect_generator_new-Copy1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jBoU8fJqdfsoOkkr1a_pXu6Amv9lMqGl
"""

import pandas as pd
import json
from textblob import TextBlob
import nltk
nltk.download('averaged_perceptron_tagger')
nltk.download('brown')
import numpy as np
from nltk.stem import PorterStemmer
porter = PorterStemmer()


input_file ='/content/drive/MyDrive/aspect_extraction/AS_dic.json'
f = open(input_file,'r')
dic = json.load(f)

substance =[]
motivation = []
clarity = []
meaningful_comparison =[]
originality = []
soundness= []
replicability= []


for keys in dic.keys():
    if dic[keys].get('substance') is not None:
        if(dic[keys]['substance'].get('positive') is not None):
            substance.extend(dic[keys]['substance']['positive'])
        if(dic[keys]['substance'].get('negative') is not None):
            substance.extend(dic[keys]['substance']['negative'])

    if dic[keys].get('motivation') is not None:
        if(dic[keys]['motivation'].get('positive') is not None):
            motivation.extend(dic[keys]['motivation']['positive'])
        if(dic[keys]['motivation'].get('negative') is not None):
            motivation.extend(dic[keys]['motivation']['negative'])
            
    if dic[keys].get('clarity') is not None:
        if(dic[keys]['clarity'].get('positive') is not None):
            clarity.extend(dic[keys]['clarity']['positive'])
        if(dic[keys]['clarity'].get('negative') is not None):
            clarity.extend(dic[keys]['clarity']['negative'])
    
    if dic[keys].get('meaningful-comparison') is not None:
        if(dic[keys]['meaningful-comparison'].get('positive') is not None):
            meaningful_comparison.extend(dic[keys]['meaningful-comparison']['positive'])
        if(dic[keys]['meaningful-comparison'].get('negative') is not None):
            meaningful_comparison.extend(dic[keys]['meaningful-comparison']['negative'])
    
    if dic[keys].get('originality') is not None:
        if(dic[keys]['originality'].get('positive') is not None):
            originality.extend(dic[keys]['originality']['positive'])
        if(dic[keys]['originality'].get('negative') is not None):
            originality.extend(dic[keys]['originality']['negative'])
            
    
    if dic[keys].get('soundness') is not None:
        if(dic[keys]['soundness'].get('positive') is not None):
            soundness.extend(dic[keys]['soundness']['positive'])
        if(dic[keys]['soundness'].get('negative') is not None):
            soundness.extend(dic[keys]['soundness']['negative'])
    
    
    if dic[keys].get('replicability') is not None:
        if(dic[keys]['replicability'].get('positive') is not None):
            replicability.extend(dic[keys]['replicability']['positive'])
        if(dic[keys]['replicability'].get('negative') is not None):
            replicability.extend(dic[keys]['replicability']['negative'])

#TFIDF

import spacy
nlp = spacy.load('en_core_web_sm')
import nltk
nltk.download('punkt')
def lemmatize(text):
  new_text=""
  sentences = nltk.sent_tokenize(text)
  for s in sentences:
  # Create a Doc object
    doc = nlp(s)
    
    # Create list of tokens from given string
    tokens = []
    for token in doc:
        tokens.append(token)
 
    lemmatized_sentence = " ".join([token.lemma_ for token in doc])
    new_text = new_text + lemmatized_sentence
  return new_text

#SKIP

'''
import pandas as pd 
from sklearn.feature_extraction.text import TfidfTransformer 
from sklearn.feature_extraction.text import CountVectorizer 

sub = lemmatize(porter.stem('. '.join(substance)))
print("1")
mot = lemmatize(porter.stem('. '.join(motivation)))
print("2")
cla = lemmatize(porter.stem('. '.join(clarity)))
print("3")
cmp = lemmatize(porter.stem('. '.join(meaningful_comparison)))
print("4")
ori = lemmatize(porter.stem('. '.join(originality)))
print("5")
sou = lemmatize(porter.stem('. '.join(soundness)))
print("6")
rep = lemmatize(porter.stem('. '.join(replicability)))                        
                            
docs= [sub,mot,cla,cmp,ori,sou,rep]
'''

def substract(test_list1,test_list2):
    res = [ ele for ele in test_list1 ]
    for a in test_list2:
        if a in test_list1:
            res.remove(a)
    return res

#FROM HERE

#instantiate CountVectorizer() 
import pandas as pd 
from sklearn.feature_extraction.text import TfidfTransformer 
from sklearn.feature_extraction.text import CountVectorizer
import gensim.downloader as api
import torch.nn.functional as F
import nltk
from nltk import word_tokenize as tk
nltk.download('punkt')
w2v = api.load('glove-wiki-gigaword-300')
#w2v = api.load('word2vec-google-news-300')

class aspect_extract:
    def __init__(self,docs):
        self.docs = docs
    def get_tfidf(self,index):
        cv=CountVectorizer(stop_words='english', ngram_range=(1,3))  #change to index,index
        #cv=CountVectorizer(ngram_range=(2,2)) 

        # this steps generates word counts for the words in your docs 
        word_count_vector=cv.fit_transform(self.docs)

        tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True) 
        tfidf_transformer.fit(word_count_vector)
        # print idf values 
        df_idf = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names(),columns=["idf_weights"]) 
        # sort ascending 
        df_idf.sort_values(by=['idf_weights'])

        # count matrix 
        count_vector=cv.transform(docs) 

        # tf-idf scores 
        tf_idf_vector=tfidf_transformer.transform(count_vector)
        feature_names = cv.get_feature_names() 
        return tf_idf_vector,feature_names

    #get tfidf vector for first document


    def give_top_k(self,index,n,thres):
        tfidf,feature_names= self.get_tfidf(n)
        first_document_vector = tfidf[index]

        #print the scores 
        df = pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=["tfidf"])
        return list(df.sort_values(by=["tfidf"],ascending=False).index)[0:thres]

    def f(self,index,n,thres = 150):
        new_list = self.give_top_k(index,n,thres)
        new_list_unique=[]
        for i in range(7):
            if(i!=index):
                new_list_unique = substract(new_list, self.give_top_k(i,n,thres))
        
        for item in new_list[0:25]:  #adding top25 strings also
          if(item not in new_list_unique):
            new_list_unique.append(item)
        return new_list_unique

asp = ["substance", "motivation", "clarity", "meaningful_comparison", "originality", "soundness", "replicability"]
aspects = [substance, motivation, clarity, meaningful_comparison, originality, soundness, replicability]

class embedding:
    def get_emb(self,word):
      if word not in w2v.vocab:
        if porter.stem(lemmatize(word)) in w2v.vocab:
           return torch.tensor(w2v.vectors[w2v.vocab[porter.stem(lemmatize(word))].index])
        else:
          return torch.zeros(w2v.vectors.shape[1])
      return torch.tensor(w2v.vectors[w2v.vocab[word].index])

        # get mean of filtered words - centroid
    def mean(self,embs):
      return torch.mean(embs, dim=0)

        # get stack of embeddings
    def emb_stack(self,word_list):
      embs = torch.stack([self.get_emb(word) for word in word_list], dim=0)
      return self.mean(embs)

create_embedding = embedding()

import torch

# aspect_extractor = aspect_extract(docs)
# #aspect_extractor.f(1,1,100)  #0th aspect , 1 gram , 200 items
# aspect_embedding = {}
# for a in asp:
#     aspect_embedding[a]={}
#     for item in aspect_extractor.f(asp.index(a),1,150):
#         aspect_embedding[a][item] = create_embedding.emb_stack(item)

# import pickle
# with open("/content/drive/MyDrive/aspect_extraction/aspect_embedding_glove_25.pickle", "wb") as outfile:
#     pickle.dump(aspect_embedding, outfile)

# Commented out IPython magic to ensure Python compatibility.
# % cd /content/drive/MyDrive/aspect_extraction

# Opening JSON file
import pickle
with open('/content/drive/MyDrive/aspect_extraction/aspect_embedding_glove.pickle','rb') as json_file:
    aspect_embedding = pickle.load(json_file)


def create_centroid(aspect_embedding):
  centroid={}
  for aspect in aspect_embedding.keys():
    mean_embedding = torch.zeros(300)
    for word in aspect_embedding[aspect].keys():
      mean_embedding.add_(aspect_embedding[aspect][word])
    mean_embedding.div_(len(aspect_embedding[aspect]))
    centroid[aspect] = mean_embedding
  return centroid

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

count = 0
asp_to_idx = {}
idx_to_asp = {}
for a in asp:
    idx_to_asp[count] = str(a)
    asp_to_idx[str(a)] = count
    count += 1



def return_filter_token(example_sent):

    stop_words = set(stopwords.words('english'))

    word_tokens = word_tokenize(example_sent)

    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]

    filtered_sentence = []

    for w in word_tokens:
        if w not in stop_words and len(w)>2:
            filtered_sentence.append(w)

    return filtered_sentence

def words_embed(wordlist):
    em = []
    for item in wordlist:
        em.append(create_embedding.emb_stack(item))
    return em


from aspect_extraction import sentiword_score as sentiword
from statistics import mean
t = sentiword.SentimentAnalysis(filename='/content/drive/MyDrive/aspect_extraction/SentiWordNet_3.0.0.txt',weighting='geometric')
def func1(aspect,sentiment, sentence):
    #sentence = lemmatize(sentence)
    f1_word_list = return_filter_token(sentence)
    embed_b = words_embed(f1_word_list)
    words = []
    last_score = 0
    last_word = ""
    senti_dic = t.score_sentence(sentence)
    for i in range(len(embed_b)):
      b = embed_b[i]
      for word in aspect_embedding[aspect].keys():
            if(f1_word_list[i] in words):
              continue

            f1_sim = F.cosine_similarity(torch.stack([b],dim=0), torch.stack([aspect_embedding[aspect][word]],dim=0))
            if f1_word_list[i] in senti_dic.keys() and senti_dic[f1_word_list[i]] is not None:
              f1_sim1 = float(f1_sim[0])
              f1_sim2 = abs(senti_dic[f1_word_list[i]])
            else:
              f1_sim1 = float(f1_sim[0])
              f1_sim2 = 0
            if( f1_sim1>0.95 or f1_sim2>0.4):
                words.append(f1_word_list[i])
                continue
    return words

def func3(aspect, phrases,topk):
  if(len(phrases)<=topk):
    return phrases

  filtered = []
  for sentence in phrases:
    flag = False

    f1_word_list = return_filter_token(sentence)
    embed_b = words_embed(f1_word_list)
    words = []
    senti_dic = t.score_sentence(sentence)
    centroids = create_centroid(aspect_embedding)
    if(aspect =="meaningful-comparison"):
        a = centroids["meaningful_comparison"]
    else:
        a = centroids[aspect]
    
    senti_dic = t.score_sentence(sentence)
    for i in range(len(embed_b)):
      b = embed_b[i]
      f1_sim = F.cosine_similarity(torch.stack([b],dim=0), torch.stack([a],dim=0))
      if f1_word_list[i] in senti_dic.keys() and senti_dic[f1_word_list[i]] is not None:
        f1_sim1 = float(f1_sim[0])
        f1_sim2 = abs(senti_dic[f1_word_list[i]])
      else:
        f1_sim1 = float(f1_sim[0])
        f1_sim2 = 0

      if(f1_sim1>0.93 or f1_sim2>0.4 or (f1_sim1*0.6 + f1_sim2*0.4)>0.6):
          flag = True
    
    if(flag == True):
      filtered.append(sentence)
      
  return filtered

def func2(aspect, sentence):
    #sentence = lemmatize(sentence)
    last_word = ""
    last_score = 0
    f1_word_list = return_filter_token(sentence)
    embed_b = words_embed(f1_word_list)
    words = []
    senti_dic = t.score_sentence(sentence)
    centroids = create_centroid(aspect_embedding)
    a = centroids[aspect]
    for i in range(len(embed_b)):
      b = embed_b[i]
      if(f1_word_list[i] in words):
          continue
      f1_sim = F.cosine_similarity(torch.stack([b],dim=0), torch.stack([a],dim=0))
      if f1_word_list[i] in senti_dic.keys() and senti_dic[f1_word_list[i]] is not None:
        f1_sim1 = float(f1_sim[0])
        f1_sim2 = abs(senti_dic[f1_word_list[i]])
      else:
        f1_sim1 = float(f1_sim[0])
        f1_sim2 = 0
      print(f1_word_list[i])
      print(f1_sim)
      print(f1_sim2)
      print("----------------------")
      if (f1_sim1>last_score == 0):
        last_score = f1_sim1
        last_word = f1_word_list[i]

      if(f1_sim1>0.93 or f1_sim2>0.4 or (f1_sim1*0.6 + f1_sim2*0.4)>0.6):
          words.append(f1_word_list[i])
          continue
    if(len(words)==0):
      words = [last_word]
    return words

#noun counts